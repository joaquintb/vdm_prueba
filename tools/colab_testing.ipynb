{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Auxiliary IYPNB for GPU Access"
      ],
      "metadata": {
        "id": "MtmhLMWgSus9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set-Up"
      ],
      "metadata": {
        "id": "4sY9RvyDbHgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content\""
      ],
      "metadata": {
        "id": "RhBr0KAdnYJ3",
        "outputId": "85039cc8-5962-4b1a-9892-74f50959f5eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bCgNVI5avfj",
        "outputId": "d278ed74-82a5-4e37-ea61-6fb4748d1b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vdm_prueba'...\n",
            "remote: Enumerating objects: 401, done.\u001b[K\n",
            "remote: Counting objects: 100% (401/401), done.\u001b[K\n",
            "remote: Compressing objects: 100% (345/345), done.\u001b[K\n",
            "remote: Total 401 (delta 77), reused 363 (delta 42), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (401/401), 5.70 MiB | 39.45 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/joaquintb/vdm_prueba.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"vdm_prueba\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDXoTl47n_b5",
        "outputId": "448cd877-3d7a-4014-97db-0b8dcab879cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/vdm_prueba\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U \\\n",
        "  diffusers==0.36.0 \\\n",
        "  accelerate==1.12.0 \\\n",
        "  peft==0.18.1 \\\n",
        "  transformers==4.57.6 \\\n",
        "  open-clip-torch==2.23.0 \\\n",
        "  medmnist==3.0.2 \\\n",
        "  pandas scikit-learn tqdm safetensors"
      ],
      "metadata": {
        "id": "ibIVQnMM2Kwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import torch; print('CUDA available:', torch.cuda.is_available()); print('Num GPUs:', torch.cuda.device_count()); print('GPU name:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtY1xESc5DWn",
        "outputId": "25fb6ac0-faa7-4e9d-cacd-1b55817bb6de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Num GPUs: 1\n",
            "GPU name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import accelerate, diffusers, transformers, peft, datasets; print('OK'); print('diffusers', diffusers.__version__); print('transformers', transformers.__version__); print('peft', peft.__version__)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I5rCAIe2Rll",
        "outputId": "79bf6bc8-50e6-4761-8abf-cad92e4d5ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-18 13:50:56.914254: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768744256.938405    1186 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768744256.944808    1186 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768744256.960160    1186 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768744256.960186    1186 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768744256.960193    1186 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768744256.960200    1186 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-18 13:50:56.964971: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "OK\n",
            "diffusers 0.36.0\n",
            "transformers 4.57.6\n",
            "peft 0.18.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare and Label"
      ],
      "metadata": {
        "id": "oPnRPXnnkrXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.dataset.prepare_and_label"
      ],
      "metadata": {
        "id": "7Cpl71tIkuQV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b7bf013-1b98-4c49-dffb-553ee583fd57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:49: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "100% 4.17M/4.17M [00:02<00:00, 2.02MB/s]\n",
            "Labelling train split:   0% 0/148 [00:00<?, ?batch/s]\n",
            "open_clip_pytorch_model.bin:   0% 0.00/784M [00:00<?, ?B/s]\u001b[A\n",
            "open_clip_pytorch_model.bin:   6% 46.1M/784M [00:02<00:46, 15.7MB/s]\u001b[A\n",
            "open_clip_pytorch_model.bin:  14% 113M/784M [00:03<00:14, 44.8MB/s] \u001b[A\n",
            "open_clip_pytorch_model.bin:  23% 180M/784M [00:03<00:07, 75.8MB/s]\u001b[A\n",
            "open_clip_pytorch_model.bin:  32% 247M/784M [00:03<00:04, 113MB/s] \u001b[A\n",
            "open_clip_pytorch_model.bin:  49% 381M/784M [00:03<00:02, 200MB/s]\u001b[A\n",
            "open_clip_pytorch_model.bin:  57% 448M/784M [00:04<00:01, 215MB/s]\u001b[A\n",
            "open_clip_pytorch_model.bin:  66% 515M/784M [00:04<00:01, 238MB/s]\u001b[A\n",
            "open_clip_pytorch_model.bin:  74% 583M/784M [00:04<00:00, 237MB/s]\u001b[A\n",
            "open_clip_pytorch_model.bin:  83% 650M/784M [00:04<00:00, 245MB/s]\u001b[A\n",
            "open_clip_pytorch_model.bin:  91% 717M/784M [00:05<00:00, 246MB/s]\u001b[A\n",
            "open_clip_pytorch_model.bin: 100% 784M/784M [00:07<00:00, 110MB/s] \n",
            "\n",
            "open_clip_config.json: 100% 707/707 [00:00<00:00, 5.00MB/s]\n",
            "\n",
            "config.json: 100% 385/385 [00:00<00:00, 3.06MB/s]\n",
            "2026-01-18 10:27:26.807957: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768732046.833269    5169 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768732046.839441    5169 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768732046.854494    5169 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768732046.854523    5169 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768732046.854527    5169 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768732046.854530    5169 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-18 10:27:26.859345: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "tokenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 208kB/s]\n",
            "\n",
            "vocab.txt: 225kB [00:00, 19.7MB/s]\n",
            "Labelling train split: 100% 148/148 [01:17<00:00,  1.90batch/s, rows=4708, subset=200/200]\n",
            "Labelling val split: 100% 17/17 [00:06<00:00,  2.60batch/s, rows=5232, subset=-]\n",
            "Labelling test split: 100% 20/20 [00:07<00:00,  2.52batch/s, rows=5856, subset=-]\n",
            "Saved: results/labeled_dataset.csv (5856 rows)\n",
            "Saved: results/metrics/auto_label_metrics.json\n",
            "Saved LoRA subset to: ./data/subset (normal=100, pneumonia=100, total=200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LoRa Fine-Tuning"
      ],
      "metadata": {
        "id": "j7XyEMeA3PBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.generation.lora_fine_tuning"
      ],
      "metadata": {
        "id": "F4QyR0uI3UG7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762e16c3-c410-47b2-baae-12eb11e1af6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running LoRA fine-tuning (official Diffusers script):\n",
            "\n",
            "accelerate launch third_party/diffusers/train_text_to_image_lora.py --pretrained_model_name_or_path runwayml/stable-diffusion-v1-5 --train_data_dir data/subset --resolution 256 --train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 0.0001 --max_train_steps 200 --rank 8 --seed 42 --output_dir results/lora/pneumonia_lora --mixed_precision fp16\n",
            "\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2026-01-18 11:39:09.246792: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768736349.509081    5012 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768736349.582456    5012 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768736350.128224    5012 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768736350.128269    5012 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768736350.128273    5012 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768736350.128278    5012 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-18 11:39:10.178044: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "INFO:__main__:Distributed environment: DistributedType.NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: fp16\n",
            "\n",
            "scheduler_config.json: 100% 308/308 [00:00<00:00, 2.09MB/s]\n",
            "{'clip_sample_range', 'rescale_betas_zero_snr', 'prediction_type', 'dynamic_thresholding_ratio', 'timestep_spacing', 'variance_type', 'sample_max_value', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
            "tokenizer_config.json: 100% 806/806 [00:00<00:00, 6.15MB/s]\n",
            "vocab.json: 1.06MB [00:00, 46.3MB/s]\n",
            "merges.txt: 525kB [00:00, 108MB/s]\n",
            "special_tokens_map.json: 100% 472/472 [00:00<00:00, 3.00MB/s]\n",
            "config.json: 100% 617/617 [00:00<00:00, 3.60MB/s]\n",
            "text_encoder/model.safetensors: 100% 492M/492M [00:03<00:00, 126MB/s]\n",
            "config.json: 100% 547/547 [00:00<00:00, 4.03MB/s]\n",
            "vae/diffusion_pytorch_model.safetensors: 100% 335M/335M [00:06<00:00, 48.6MB/s]\n",
            "{'use_post_quant_conv', 'scaling_factor', 'use_quant_conv', 'latents_std', 'latents_mean', 'shift_factor', 'mid_block_add_attention', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at runwayml/stable-diffusion-v1-5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "config.json: 100% 743/743 [00:00<00:00, 5.52MB/s]\n",
            "unet/diffusion_pytorch_model.safetensors: 100% 3.44G/3.44G [01:23<00:00, 41.2MB/s]\n",
            "{'resnet_time_scale_shift', 'encoder_hid_dim_type', 'time_embedding_type', 'class_embed_type', 'num_attention_heads', 'encoder_hid_dim', 'upcast_attention', 'addition_embed_type_num_heads', 'dual_cross_attention', 'num_class_embeds', 'attention_type', 'time_cond_proj_dim', 'class_embeddings_concat', 'time_embedding_act_fn', 'mid_block_type', 'conv_in_kernel', 'cross_attention_norm', 'only_cross_attention', 'addition_embed_type', 'resnet_skip_time_act', 'reverse_transformer_layers_per_block', 'dropout', 'timestep_post_act', 'addition_time_embed_dim', 'time_embedding_dim', 'use_linear_projection', 'resnet_out_scale_factor', 'mid_block_only_cross_attention', 'conv_out_kernel', 'transformer_layers_per_block', 'projection_class_embeddings_input_dim'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
            "\n",
            "All the weights of UNet2DConditionModel were initialized from the model checkpoint at runwayml/stable-diffusion-v1-5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
            "Resolving data files: 100% 201/201 [00:00<00:00, 20324.38it/s]\n",
            "Downloading data: 100% 201/201 [00:00<00:00, 32068.74files/s]\n",
            "Generating train split: 200 examples [00:00, 11734.45 examples/s]\n",
            "Parameter 'transform'=<function main.<locals>.preprocess_train at 0x7811171b7c40> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "WARNING:datasets.fingerprint:Parameter 'transform'=<function main.<locals>.preprocess_train at 0x7811171b7c40> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "INFO:__main__:***** Running training *****\n",
            "INFO:__main__:  Num examples = 200\n",
            "INFO:__main__:  Num Epochs = 4\n",
            "INFO:__main__:  Instantaneous batch size per device = 1\n",
            "INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "INFO:__main__:  Gradient Accumulation steps = 4\n",
            "INFO:__main__:  Total optimization steps = 200\n",
            "Steps: 100% 200/200 [02:45<00:00,  1.10it/s, lr=0.0001, step_loss=0.0238]Model weights saved in results/lora/pneumonia_lora/pytorch_lora_weights.safetensors\n",
            "Steps: 100% 200/200 [02:45<00:00,  1.21it/s, lr=0.0001, step_loss=0.0238]\n",
            "\n",
            "Saved LoRA adapter to: results/lora/pneumonia_lora\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LCM"
      ],
      "metadata": {
        "id": "cT0kLS-xbOt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.generation.lcm_generate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W05hM-NveFNZ",
        "outputId": "180f6b75-bd72-45e7-a33c-522dbd62d607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-18 12:02:37.993169: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768737758.017845   10912 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768737758.028473   10912 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768737758.060367   10912 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768737758.060393   10912 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768737758.060399   10912 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768737758.060403   10912 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-18 12:02:38.065024: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "model_index.json: 100% 541/541 [00:00<00:00, 3.43MB/s]\n",
            "Fetching 13 files:   0% 0/13 [00:00<?, ?it/s]\n",
            "preprocessor_config.json: 100% 342/342 [00:00<00:00, 3.28MB/s]\n",
            "Fetching 13 files: 100% 13/13 [00:00<00:00, 48.61it/s]\n",
            "Loading pipeline components...:  33% 2/6 [00:00<00:00, 18.62it/s]`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading pipeline components...: 100% 6/6 [00:06<00:00,  1.10s/it]\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "pytorch_lora_weights.safetensors: 100% 135M/135M [00:01<00:00, 69.6MB/s]\n",
            "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:285: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n",
            "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
            "Generating images:   0% 0/5 [00:00<?, ?batch/s]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 1/4 [00:01<00:04,  1.57s/it]\u001b[A\n",
            " 50% 2/4 [00:01<00:01,  1.29it/s]\u001b[A\n",
            " 75% 3/4 [00:02<00:00,  1.91it/s]\u001b[A\n",
            "100% 4/4 [00:02<00:00,  1.79it/s]\n",
            "Generating images:  20% 1/5 [00:03<00:14,  3.53s/batch]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 1/4 [00:00<00:00,  4.54it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  4.52it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  4.52it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  4.51it/s]\n",
            "Generating images:  40% 2/5 [00:05<00:06,  2.33s/batch]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 1/4 [00:00<00:00,  4.56it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  4.53it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  4.52it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  4.51it/s]\n",
            "Generating images:  60% 3/5 [00:06<00:03,  1.95s/batch]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 1/4 [00:00<00:00,  4.59it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  4.53it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  4.54it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  4.52it/s]\n",
            "Generating images:  80% 4/5 [00:08<00:01,  1.77s/batch]\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 1/4 [00:00<00:00,  4.57it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  4.50it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  4.47it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  4.47it/s]\n",
            "Generating images: 100% 5/5 [00:09<00:00,  1.90s/batch]\n",
            "Saved 10 images to: ./results/generated_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the Pipeline"
      ],
      "metadata": {
        "id": "SYf4FB81X9XF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.pipeline --force"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVH8jDdIX_TC",
        "outputId": "fb2faa8c-6d9a-4878-d8b0-2100d47868a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:49: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "2026-01-18 13:52:18.464298: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768744338.482381    1561 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768744338.488254    1561 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768744338.503040    1561 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768744338.503065    1561 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768744338.503069    1561 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768744338.503072    1561 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-18 13:52:18.507527: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "Device: cuda\n",
            "\n",
            "[1/3] Running prepare_and_label...\n",
            "100% 4.17M/4.17M [00:00<00:00, 5.04MB/s]\n",
            "Labelling train split:   0% 0/148 [00:00<?, ?batch/s]\n",
            "open_clip_pytorch_model.bin:   0% 0.00/784M [00:00<?, ?B/s]\u001b[A\n",
            "open_clip_pytorch_model.bin:   6% 46.1M/784M [00:02<00:35, 20.6MB/s]\u001b[A\n",
            "open_clip_pytorch_model.bin:  14% 113M/784M [00:02<00:13, 48.0MB/s] \u001b[A\n",
            "open_clip_pytorch_model.bin:  23% 180M/784M [00:02<00:07, 84.8MB/s]\u001b[A\n",
            "open_clip_pytorch_model.bin:  32% 247M/784M [00:03<00:04, 123MB/s] \u001b[A\n",
            "open_clip_pytorch_model.bin:  40% 314M/784M [00:03<00:02, 176MB/s]\u001b[A\n",
            "open_clip_pytorch_model.bin:  49% 381M/784M [00:03<00:01, 222MB/s]\u001b[A\n",
            "open_clip_pytorch_model.bin:  57% 448M/784M [00:05<00:04, 75.1MB/s]\u001b[A\n",
            "open_clip_pytorch_model.bin:  74% 582M/784M [00:05<00:01, 130MB/s] \u001b[A\n",
            "open_clip_pytorch_model.bin:  91% 717M/784M [00:05<00:00, 191MB/s]\u001b[A\n",
            "open_clip_pytorch_model.bin: 100% 784M/784M [00:06<00:00, 127MB/s]\n",
            "\n",
            "open_clip_config.json: 100% 707/707 [00:00<00:00, 5.63MB/s]\n",
            "\n",
            "config.json: 100% 385/385 [00:00<00:00, 2.71MB/s]\n",
            "\n",
            "tokenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 221kB/s]\n",
            "\n",
            "vocab.txt: 225kB [00:00, 45.9MB/s]\n",
            "Labelling train split: 100% 148/148 [01:09<00:00,  2.12batch/s, rows=4708, subset=500/500]\n",
            "Labelling val split: 100% 17/17 [00:06<00:00,  2.61batch/s, rows=5232, subset=-]\n",
            "Labelling test split: 100% 20/20 [00:07<00:00,  2.66batch/s, rows=5856, subset=-]\n",
            "Saved: results/labeled_dataset.csv (5856 rows)\n",
            "Saved: results/metrics/auto_label_metrics.json\n",
            "Saved LoRA subset to: ./data/subset (normal=250, pneumonia=250, total=500)\n",
            "Found metrics: results/metrics/auto_label_metrics.json\n",
            "\n",
            "[2/3] Running LoRA fine-tuning...\n",
            "\n",
            "Running LoRA fine-tuning (official Diffusers script):\n",
            "\n",
            "accelerate launch third_party/diffusers/train_text_to_image_lora.py --pretrained_model_name_or_path runwayml/stable-diffusion-v1-5 --train_data_dir data/subset --resolution 256 --train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 0.0001 --max_train_steps 200 --rank 8 --seed 42 --output_dir results/lora/pneumonia_lora --mixed_precision fp16\n",
            "\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2026-01-18 13:54:04.043012: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768744444.076766    2097 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768744444.086754    2097 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768744444.110949    2097 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768744444.110986    2097 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768744444.110996    2097 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768744444.111021    2097 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "INFO:__main__:Distributed environment: DistributedType.NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: fp16\n",
            "\n",
            "scheduler_config.json: 100% 308/308 [00:00<00:00, 2.06MB/s]\n",
            "{'clip_sample_range', 'dynamic_thresholding_ratio', 'thresholding', 'timestep_spacing', 'sample_max_value', 'rescale_betas_zero_snr', 'variance_type', 'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "tokenizer_config.json: 100% 806/806 [00:00<00:00, 5.40MB/s]\n",
            "vocab.json: 1.06MB [00:00, 58.2MB/s]\n",
            "merges.txt: 525kB [00:00, 134MB/s]\n",
            "special_tokens_map.json: 100% 472/472 [00:00<00:00, 3.25MB/s]\n",
            "config.json: 100% 617/617 [00:00<00:00, 4.24MB/s]\n",
            "text_encoder/model.safetensors: 100% 492M/492M [00:02<00:00, 167MB/s]\n",
            "config.json: 100% 547/547 [00:00<00:00, 4.03MB/s]\n",
            "vae/diffusion_pytorch_model.safetensors: 100% 335M/335M [00:01<00:00, 194MB/s]\n",
            "{'shift_factor', 'use_quant_conv', 'use_post_quant_conv', 'mid_block_add_attention', 'force_upcast', 'scaling_factor', 'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at runwayml/stable-diffusion-v1-5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "config.json: 100% 743/743 [00:00<00:00, 5.81MB/s]\n",
            "unet/diffusion_pytorch_model.safetensors: 100% 3.44G/3.44G [00:21<00:00, 157MB/s]\n",
            "{'addition_embed_type', 'cross_attention_norm', 'addition_embed_type_num_heads', 'time_embedding_act_fn', 'use_linear_projection', 'time_embedding_dim', 'only_cross_attention', 'reverse_transformer_layers_per_block', 'transformer_layers_per_block', 'time_embedding_type', 'timestep_post_act', 'class_embeddings_concat', 'dual_cross_attention', 'encoder_hid_dim', 'conv_out_kernel', 'upcast_attention', 'addition_time_embed_dim', 'resnet_time_scale_shift', 'resnet_out_scale_factor', 'encoder_hid_dim_type', 'num_attention_heads', 'time_cond_proj_dim', 'dropout', 'resnet_skip_time_act', 'mid_block_type', 'conv_in_kernel', 'projection_class_embeddings_input_dim', 'num_class_embeds', 'attention_type', 'mid_block_only_cross_attention', 'class_embed_type'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
            "\n",
            "All the weights of UNet2DConditionModel were initialized from the model checkpoint at runwayml/stable-diffusion-v1-5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
            "Resolving data files: 100% 501/501 [00:00<00:00, 18048.77it/s]\n",
            "Downloading data: 100% 501/501 [00:00<00:00, 23692.63files/s]\n",
            "Generating train split: 500 examples [00:00, 10056.06 examples/s]\n",
            "INFO:__main__:***** Running training *****\n",
            "INFO:__main__:  Num examples = 500\n",
            "INFO:__main__:  Num Epochs = 2\n",
            "INFO:__main__:  Instantaneous batch size per device = 1\n",
            "INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "INFO:__main__:  Gradient Accumulation steps = 4\n",
            "INFO:__main__:  Total optimization steps = 200\n",
            "Steps: 100% 200/200 [02:37<00:00,  1.23it/s, lr=0.0001, step_loss=0.018]Model weights saved in results/lora/pneumonia_lora/pytorch_lora_weights.safetensors\n",
            "Steps: 100% 200/200 [02:37<00:00,  1.27it/s, lr=0.0001, step_loss=0.018]\n",
            "\n",
            "Saved LoRA adapter to: results/lora/pneumonia_lora\n",
            "\n",
            "Using LoRA weights: results/lora/pneumonia_lora/pytorch_lora_weights.safetensors\n",
            "\n",
            "[3/3] Generating images with LCM...\n",
            "model_index.json: 100% 541/541 [00:00<00:00, 5.29MB/s]\n",
            "Fetching 13 files:   0% 0/13 [00:00<?, ?it/s]\n",
            "preprocessor_config.json: 100% 342/342 [00:00<00:00, 2.98MB/s]\n",
            "Fetching 13 files: 100% 13/13 [00:00<00:00, 41.92it/s]\n",
            "Loading pipeline components...:   0% 0/6 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading pipeline components...: 100% 6/6 [00:08<00:00,  1.37s/it]\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "pytorch_lora_weights.safetensors: 100% 135M/135M [00:02<00:00, 51.1MB/s]\n",
            "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:285: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n",
            "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
            "Generating images:   0% 0/15 [00:00<?, ?batch/s]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 1/6 [00:01<00:05,  1.02s/it]\u001b[A\n",
            " 33% 2/6 [00:01<00:02,  1.80it/s]\u001b[A\n",
            " 50% 3/6 [00:01<00:01,  2.45it/s]\u001b[A\n",
            " 67% 4/6 [00:01<00:00,  2.94it/s]\u001b[A\n",
            " 83% 5/6 [00:01<00:00,  3.27it/s]\u001b[A\n",
            "100% 6/6 [00:02<00:00,  2.72it/s]\n",
            "Generating images:   7% 1/15 [00:03<00:44,  3.16s/batch]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 1/6 [00:00<00:01,  4.29it/s]\u001b[A\n",
            " 33% 2/6 [00:00<00:00,  4.25it/s]\u001b[A\n",
            " 50% 3/6 [00:00<00:00,  4.26it/s]\u001b[A\n",
            " 67% 4/6 [00:00<00:00,  4.15it/s]\u001b[A\n",
            " 83% 5/6 [00:01<00:00,  4.18it/s]\u001b[A\n",
            "100% 6/6 [00:01<00:00,  4.21it/s]\n",
            "Generating images:  13% 2/15 [00:05<00:32,  2.53s/batch]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 1/6 [00:00<00:01,  4.32it/s]\u001b[A\n",
            " 33% 2/6 [00:00<00:00,  4.28it/s]\u001b[A\n",
            " 50% 3/6 [00:00<00:00,  4.24it/s]\u001b[A\n",
            " 67% 4/6 [00:00<00:00,  4.15it/s]\u001b[A\n",
            " 83% 5/6 [00:01<00:00,  4.15it/s]\u001b[A\n",
            "100% 6/6 [00:01<00:00,  4.19it/s]\n",
            "Generating images:  20% 3/15 [00:07<00:27,  2.32s/batch]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 1/6 [00:00<00:01,  4.32it/s]\u001b[A\n",
            " 33% 2/6 [00:00<00:00,  4.26it/s]\u001b[A\n",
            " 50% 3/6 [00:00<00:00,  4.23it/s]\u001b[A\n",
            " 67% 4/6 [00:00<00:00,  4.09it/s]\u001b[A\n",
            " 83% 5/6 [00:01<00:00,  4.13it/s]\u001b[A\n",
            "100% 6/6 [00:01<00:00,  4.17it/s]\n",
            "Generating images:  27% 4/15 [00:09<00:24,  2.23s/batch]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 1/6 [00:00<00:01,  4.29it/s]\u001b[A\n",
            " 33% 2/6 [00:00<00:00,  4.23it/s]\u001b[A\n",
            " 50% 3/6 [00:00<00:00,  4.21it/s]\u001b[A\n",
            " 67% 4/6 [00:00<00:00,  4.08it/s]\u001b[A\n",
            " 83% 5/6 [00:01<00:00,  4.12it/s]\u001b[A\n",
            "100% 6/6 [00:01<00:00,  4.15it/s]\n",
            "Generating images:  33% 5/15 [00:11<00:21,  2.18s/batch]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 1/6 [00:00<00:01,  4.30it/s]\u001b[A\n",
            " 33% 2/6 [00:00<00:00,  4.25it/s]\u001b[A\n",
            " 50% 3/6 [00:00<00:00,  4.23it/s]\u001b[A\n",
            " 67% 4/6 [00:00<00:00,  4.12it/s]\u001b[A\n",
            " 83% 5/6 [00:01<00:00,  4.12it/s]\u001b[A\n",
            "100% 6/6 [00:01<00:00,  4.16it/s]\n",
            "Generating images:  40% 6/15 [00:13<00:19,  2.16s/batch]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 1/6 [00:00<00:01,  4.25it/s]\u001b[A\n",
            " 33% 2/6 [00:00<00:00,  4.20it/s]\u001b[A\n",
            " 50% 3/6 [00:00<00:00,  4.19it/s]\u001b[A\n",
            " 67% 4/6 [00:00<00:00,  4.05it/s]\u001b[A\n",
            " 83% 5/6 [00:01<00:00,  4.05it/s]\u001b[A\n",
            "100% 6/6 [00:01<00:00,  4.11it/s]\n",
            "Generating images:  47% 7/15 [00:15<00:17,  2.15s/batch]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 1/6 [00:00<00:01,  4.26it/s]\u001b[A\n",
            " 33% 2/6 [00:00<00:00,  4.22it/s]\u001b[A\n",
            " 50% 3/6 [00:00<00:00,  4.21it/s]\u001b[A\n",
            " 67% 4/6 [00:00<00:00,  4.08it/s]\u001b[A\n",
            " 83% 5/6 [00:01<00:00,  4.10it/s]\u001b[A\n",
            "100% 6/6 [00:01<00:00,  4.14it/s]\n",
            "Generating images:  53% 8/15 [00:17<00:14,  2.14s/batch]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 1/6 [00:00<00:01,  4.25it/s]\u001b[A\n",
            " 33% 2/6 [00:00<00:00,  4.21it/s]\u001b[A\n",
            " 50% 3/6 [00:00<00:00,  4.19it/s]\u001b[A\n",
            " 67% 4/6 [00:00<00:00,  4.02it/s]\u001b[A\n",
            " 83% 5/6 [00:01<00:00,  4.07it/s]\u001b[A\n",
            "100% 6/6 [00:01<00:00,  4.11it/s]\n",
            "Generating images:  60% 9/15 [00:19<00:12,  2.13s/batch]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 1/6 [00:00<00:01,  4.22it/s]\u001b[A\n",
            " 33% 2/6 [00:00<00:00,  4.17it/s]\u001b[A\n",
            " 50% 3/6 [00:00<00:00,  4.16it/s]\u001b[A\n",
            " 67% 4/6 [00:00<00:00,  4.05it/s]\u001b[A\n",
            " 83% 5/6 [00:01<00:00,  4.07it/s]\u001b[A\n",
            "100% 6/6 [00:01<00:00,  4.10it/s]\n",
            "Generating images:  67% 10/15 [00:22<00:10,  2.13s/batch]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 1/6 [00:00<00:01,  4.25it/s]\u001b[A\n",
            " 33% 2/6 [00:00<00:00,  4.19it/s]\u001b[A\n",
            " 50% 3/6 [00:00<00:00,  4.15it/s]\u001b[A\n",
            " 67% 4/6 [00:00<00:00,  4.00it/s]\u001b[A\n",
            " 83% 5/6 [00:01<00:00,  4.04it/s]\u001b[A\n",
            "100% 6/6 [00:01<00:00,  4.09it/s]\n",
            "Generating images:  73% 11/15 [00:24<00:08,  2.13s/batch]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 1/6 [00:00<00:01,  4.22it/s]\u001b[A\n",
            " 33% 2/6 [00:00<00:00,  4.16it/s]\u001b[A\n",
            " 50% 3/6 [00:00<00:00,  4.15it/s]\u001b[A\n",
            " 67% 4/6 [00:00<00:00,  4.05it/s]\u001b[A\n",
            " 83% 5/6 [00:01<00:00,  4.06it/s]\u001b[A\n",
            "100% 6/6 [00:01<00:00,  4.09it/s]\n",
            "Generating images:  80% 12/15 [00:26<00:06,  2.14s/batch]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 1/6 [00:00<00:01,  4.19it/s]\u001b[A\n",
            " 33% 2/6 [00:00<00:00,  4.13it/s]\u001b[A\n",
            " 50% 3/6 [00:00<00:00,  4.12it/s]\u001b[A\n",
            " 67% 4/6 [00:00<00:00,  3.99it/s]\u001b[A\n",
            " 83% 5/6 [00:01<00:00,  4.02it/s]\u001b[A\n",
            "100% 6/6 [00:01<00:00,  4.05it/s]\n",
            "Generating images:  87% 13/15 [00:28<00:04,  2.15s/batch]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 1/6 [00:00<00:01,  4.20it/s]\u001b[A\n",
            " 33% 2/6 [00:00<00:00,  4.14it/s]\u001b[A\n",
            " 50% 3/6 [00:00<00:00,  4.13it/s]\u001b[A\n",
            " 67% 4/6 [00:00<00:00,  3.97it/s]\u001b[A\n",
            " 83% 5/6 [00:01<00:00,  4.01it/s]\u001b[A\n",
            "100% 6/6 [00:01<00:00,  4.05it/s]\n",
            "Generating images:  93% 14/15 [00:30<00:02,  2.15s/batch]\n",
            "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 1/6 [00:00<00:01,  4.21it/s]\u001b[A\n",
            " 33% 2/6 [00:00<00:00,  4.15it/s]\u001b[A\n",
            " 50% 3/6 [00:00<00:00,  4.13it/s]\u001b[A\n",
            " 67% 4/6 [00:00<00:00,  4.02it/s]\u001b[A\n",
            " 83% 5/6 [00:01<00:00,  4.04it/s]\u001b[A\n",
            "100% 6/6 [00:01<00:00,  4.08it/s]\n",
            "Generating images: 100% 15/15 [00:32<00:00,  2.19s/batch]\n",
            "Images saved to: ./results/generated_images\n",
            "\n",
            "Pipeline summary written to: results/pipeline_summary.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pushing Changes"
      ],
      "metadata": {
        "id": "KS5Fed9RiAc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Joaquin Torres\"\n",
        "!git config --global user.email \"joaquintobrw@gmail.com\""
      ],
      "metadata": {
        "id": "vMgUEJvJga_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add .\n",
        "!git commit -m \"Run whole pipeline in colab for the first time\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi0hdqzOgPuj",
        "outputId": "fb71f92c-a707-4655-f0b8-630c572daa2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main ea82d57] Run whole pipeline in colab for the first time\n",
            " 410 files changed, 368 insertions(+), 59 deletions(-)\n",
            " rewrite data/subset/images/img_00122.png (99%)\n",
            " rewrite data/subset/images/img_00123.png (99%)\n",
            " copy data/subset/images/{img_00122.png => img_00124.png} (100%)\n",
            " rewrite data/subset/images/img_00125.png (99%)\n",
            " rewrite data/subset/images/img_00126.png (99%)\n",
            " copy data/subset/images/{img_00123.png => img_00127.png} (100%)\n",
            " rename data/subset/images/{img_00124.png => img_00128.png} (100%)\n",
            " rewrite data/subset/images/img_00129.png (99%)\n",
            " rewrite data/subset/images/img_00130.png (99%)\n",
            " copy data/subset/images/{img_00125.png => img_00131.png} (100%)\n",
            " rewrite data/subset/images/img_00132.png (99%)\n",
            " copy data/subset/images/{img_00126.png => img_00133.png} (100%)\n",
            " rewrite data/subset/images/img_00134.png (99%)\n",
            " rewrite data/subset/images/img_00135.png (99%)\n",
            " rewrite data/subset/images/img_00136.png (99%)\n",
            " rename data/subset/images/{img_00127.png => img_00137.png} (100%)\n",
            " rewrite data/subset/images/img_00138.png (99%)\n",
            " rewrite data/subset/images/img_00139.png (99%)\n",
            " rewrite data/subset/images/img_00140.png (99%)\n",
            " rewrite data/subset/images/img_00141.png (99%)\n",
            " rewrite data/subset/images/img_00142.png (99%)\n",
            " rewrite data/subset/images/img_00143.png (99%)\n",
            " rewrite data/subset/images/img_00144.png (99%)\n",
            " rewrite data/subset/images/img_00145.png (99%)\n",
            " rewrite data/subset/images/img_00146.png (99%)\n",
            " rename data/subset/images/{img_00128.png => img_00147.png} (100%)\n",
            " rewrite data/subset/images/img_00148.png (99%)\n",
            " rewrite data/subset/images/img_00149.png (99%)\n",
            " rewrite data/subset/images/img_00150.png (99%)\n",
            " rewrite data/subset/images/img_00151.png (99%)\n",
            " rewrite data/subset/images/img_00152.png (99%)\n",
            " rewrite data/subset/images/img_00153.png (99%)\n",
            " rewrite data/subset/images/img_00154.png (99%)\n",
            " rewrite data/subset/images/img_00155.png (99%)\n",
            " rewrite data/subset/images/img_00156.png (99%)\n",
            " copy data/subset/images/{img_00129.png => img_00157.png} (100%)\n",
            " rewrite data/subset/images/img_00158.png (99%)\n",
            " copy data/subset/images/{img_00130.png => img_00159.png} (100%)\n",
            " rewrite data/subset/images/img_00160.png (99%)\n",
            " rewrite data/subset/images/img_00161.png (99%)\n",
            " rewrite data/subset/images/img_00162.png (99%)\n",
            " rewrite data/subset/images/img_00163.png (99%)\n",
            " rewrite data/subset/images/img_00164.png (99%)\n",
            " rewrite data/subset/images/img_00165.png (99%)\n",
            " rewrite data/subset/images/img_00166.png (99%)\n",
            " rewrite data/subset/images/img_00167.png (99%)\n",
            " rewrite data/subset/images/img_00168.png (99%)\n",
            " rewrite data/subset/images/img_00169.png (99%)\n",
            " rename data/subset/images/{img_00131.png => img_00170.png} (100%)\n",
            " rewrite data/subset/images/img_00171.png (99%)\n",
            " rewrite data/subset/images/img_00172.png (99%)\n",
            " rewrite data/subset/images/img_00173.png (99%)\n",
            " copy data/subset/images/{img_00132.png => img_00174.png} (100%)\n",
            " rewrite data/subset/images/img_00175.png (99%)\n",
            " rename data/subset/images/{img_00133.png => img_00176.png} (100%)\n",
            " rewrite data/subset/images/img_00177.png (99%)\n",
            " rewrite data/subset/images/img_00178.png (99%)\n",
            " rewrite data/subset/images/img_00179.png (99%)\n",
            " copy data/subset/images/{img_00134.png => img_00180.png} (100%)\n",
            " rewrite data/subset/images/img_00181.png (99%)\n",
            " rewrite data/subset/images/img_00182.png (99%)\n",
            " copy data/subset/images/{img_00135.png => img_00183.png} (100%)\n",
            " copy data/subset/images/{img_00136.png => img_00184.png} (100%)\n",
            " rewrite data/subset/images/img_00185.png (99%)\n",
            " rewrite data/subset/images/img_00186.png (99%)\n",
            " rename data/subset/images/{img_00137.png => img_00187.png} (100%)\n",
            " rewrite data/subset/images/img_00188.png (99%)\n",
            " rewrite data/subset/images/img_00189.png (99%)\n",
            " rewrite data/subset/images/img_00190.png (99%)\n",
            " copy data/subset/images/{img_00138.png => img_00191.png} (100%)\n",
            " rewrite data/subset/images/img_00192.png (99%)\n",
            " rewrite data/subset/images/img_00193.png (99%)\n",
            " copy data/subset/images/{img_00139.png => img_00194.png} (100%)\n",
            " rewrite data/subset/images/img_00195.png (99%)\n",
            " copy data/subset/images/{img_00140.png => img_00196.png} (100%)\n",
            " rewrite data/subset/images/img_00197.png (99%)\n",
            " rewrite data/subset/images/img_00198.png (99%)\n",
            " rewrite data/subset/images/img_00199.png (99%)\n",
            " copy data/subset/images/{img_00141.png => img_00200.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00201.png\n",
            " create mode 100644 data/subset/images/img_00202.png\n",
            " create mode 100644 data/subset/images/img_00203.png\n",
            " create mode 100644 data/subset/images/img_00204.png\n",
            " create mode 100644 data/subset/images/img_00205.png\n",
            " copy data/subset/images/{img_00142.png => img_00206.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00207.png\n",
            " create mode 100644 data/subset/images/img_00208.png\n",
            " create mode 100644 data/subset/images/img_00209.png\n",
            " create mode 100644 data/subset/images/img_00210.png\n",
            " copy data/subset/images/{img_00143.png => img_00211.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00212.png\n",
            " create mode 100644 data/subset/images/img_00213.png\n",
            " create mode 100644 data/subset/images/img_00214.png\n",
            " copy data/subset/images/{img_00144.png => img_00215.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00216.png\n",
            " copy data/subset/images/{img_00145.png => img_00217.png} (100%)\n",
            " copy data/subset/images/{img_00146.png => img_00218.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00219.png\n",
            " create mode 100644 data/subset/images/img_00220.png\n",
            " create mode 100644 data/subset/images/img_00221.png\n",
            " create mode 100644 data/subset/images/img_00222.png\n",
            " create mode 100644 data/subset/images/img_00223.png\n",
            " rename data/subset/images/{img_00147.png => img_00224.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00225.png\n",
            " create mode 100644 data/subset/images/img_00226.png\n",
            " create mode 100644 data/subset/images/img_00227.png\n",
            " create mode 100644 data/subset/images/img_00228.png\n",
            " create mode 100644 data/subset/images/img_00229.png\n",
            " create mode 100644 data/subset/images/img_00230.png\n",
            " create mode 100644 data/subset/images/img_00231.png\n",
            " create mode 100644 data/subset/images/img_00232.png\n",
            " copy data/subset/images/{img_00148.png => img_00233.png} (100%)\n",
            " copy data/subset/images/{img_00149.png => img_00234.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00235.png\n",
            " copy data/subset/images/{img_00150.png => img_00236.png} (100%)\n",
            " copy data/subset/images/{img_00151.png => img_00237.png} (100%)\n",
            " copy data/subset/images/{img_00152.png => img_00238.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00239.png\n",
            " create mode 100644 data/subset/images/img_00240.png\n",
            " create mode 100644 data/subset/images/img_00241.png\n",
            " copy data/subset/images/{img_00153.png => img_00242.png} (100%)\n",
            " copy data/subset/images/{img_00154.png => img_00243.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00244.png\n",
            " create mode 100644 data/subset/images/img_00245.png\n",
            " create mode 100644 data/subset/images/img_00246.png\n",
            " create mode 100644 data/subset/images/img_00247.png\n",
            " create mode 100644 data/subset/images/img_00248.png\n",
            " create mode 100644 data/subset/images/img_00249.png\n",
            " create mode 100644 data/subset/images/img_00250.png\n",
            " create mode 100644 data/subset/images/img_00251.png\n",
            " create mode 100644 data/subset/images/img_00252.png\n",
            " create mode 100644 data/subset/images/img_00253.png\n",
            " copy data/subset/images/{img_00155.png => img_00254.png} (100%)\n",
            " copy data/subset/images/{img_00156.png => img_00255.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00256.png\n",
            " create mode 100644 data/subset/images/img_00257.png\n",
            " rename data/subset/images/{img_00157.png => img_00258.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00259.png\n",
            " create mode 100644 data/subset/images/img_00260.png\n",
            " create mode 100644 data/subset/images/img_00261.png\n",
            " copy data/subset/images/{img_00158.png => img_00262.png} (100%)\n",
            " rename data/subset/images/{img_00159.png => img_00263.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00264.png\n",
            " create mode 100644 data/subset/images/img_00265.png\n",
            " copy data/subset/images/{img_00160.png => img_00266.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00267.png\n",
            " create mode 100644 data/subset/images/img_00268.png\n",
            " create mode 100644 data/subset/images/img_00269.png\n",
            " copy data/subset/images/{img_00161.png => img_00270.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00271.png\n",
            " create mode 100644 data/subset/images/img_00272.png\n",
            " create mode 100644 data/subset/images/img_00273.png\n",
            " create mode 100644 data/subset/images/img_00274.png\n",
            " copy data/subset/images/{img_00162.png => img_00275.png} (100%)\n",
            " copy data/subset/images/{img_00163.png => img_00276.png} (100%)\n",
            " copy data/subset/images/{img_00164.png => img_00277.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00278.png\n",
            " copy data/subset/images/{img_00165.png => img_00279.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00280.png\n",
            " copy data/subset/images/{img_00166.png => img_00281.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00282.png\n",
            " copy data/subset/images/{img_00167.png => img_00283.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00284.png\n",
            " create mode 100644 data/subset/images/img_00285.png\n",
            " create mode 100644 data/subset/images/img_00286.png\n",
            " create mode 100644 data/subset/images/img_00287.png\n",
            " create mode 100644 data/subset/images/img_00288.png\n",
            " create mode 100644 data/subset/images/img_00289.png\n",
            " create mode 100644 data/subset/images/img_00290.png\n",
            " create mode 100644 data/subset/images/img_00291.png\n",
            " create mode 100644 data/subset/images/img_00292.png\n",
            " create mode 100644 data/subset/images/img_00293.png\n",
            " create mode 100644 data/subset/images/img_00294.png\n",
            " create mode 100644 data/subset/images/img_00295.png\n",
            " create mode 100644 data/subset/images/img_00296.png\n",
            " create mode 100644 data/subset/images/img_00297.png\n",
            " create mode 100644 data/subset/images/img_00298.png\n",
            " create mode 100644 data/subset/images/img_00299.png\n",
            " copy data/subset/images/{img_00168.png => img_00300.png} (100%)\n",
            " copy data/subset/images/{img_00169.png => img_00301.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00302.png\n",
            " rename data/subset/images/{img_00170.png => img_00303.png} (100%)\n",
            " copy data/subset/images/{img_00171.png => img_00304.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00305.png\n",
            " create mode 100644 data/subset/images/img_00306.png\n",
            " create mode 100644 data/subset/images/img_00307.png\n",
            " create mode 100644 data/subset/images/img_00308.png\n",
            " copy data/subset/images/{img_00172.png => img_00309.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00310.png\n",
            " create mode 100644 data/subset/images/img_00311.png\n",
            " create mode 100644 data/subset/images/img_00312.png\n",
            " create mode 100644 data/subset/images/img_00313.png\n",
            " copy data/subset/images/{img_00173.png => img_00314.png} (100%)\n",
            " rename data/subset/images/{img_00174.png => img_00315.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00316.png\n",
            " copy data/subset/images/{img_00175.png => img_00317.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00318.png\n",
            " rename data/subset/images/{img_00176.png => img_00319.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00320.png\n",
            " create mode 100644 data/subset/images/img_00321.png\n",
            " create mode 100644 data/subset/images/img_00322.png\n",
            " create mode 100644 data/subset/images/img_00323.png\n",
            " create mode 100644 data/subset/images/img_00324.png\n",
            " create mode 100644 data/subset/images/img_00325.png\n",
            " copy data/subset/images/{img_00177.png => img_00326.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00327.png\n",
            " copy data/subset/images/{img_00178.png => img_00328.png} (100%)\n",
            " copy data/subset/images/{img_00179.png => img_00329.png} (100%)\n",
            " rename data/subset/images/{img_00180.png => img_00330.png} (100%)\n",
            " copy data/subset/images/{img_00181.png => img_00331.png} (100%)\n",
            " copy data/subset/images/{img_00182.png => img_00332.png} (100%)\n",
            " rename data/subset/images/{img_00183.png => img_00333.png} (100%)\n",
            " rename data/subset/images/{img_00184.png => img_00334.png} (100%)\n",
            " copy data/subset/images/{img_00185.png => img_00335.png} (100%)\n",
            " copy data/subset/images/{img_00186.png => img_00336.png} (100%)\n",
            " rename data/subset/images/{img_00187.png => img_00337.png} (100%)\n",
            " copy data/subset/images/{img_00188.png => img_00338.png} (100%)\n",
            " copy data/subset/images/{img_00189.png => img_00339.png} (100%)\n",
            " copy data/subset/images/{img_00190.png => img_00340.png} (100%)\n",
            " rename data/subset/images/{img_00191.png => img_00341.png} (100%)\n",
            " copy data/subset/images/{img_00192.png => img_00342.png} (100%)\n",
            " copy data/subset/images/{img_00193.png => img_00343.png} (100%)\n",
            " rename data/subset/images/{img_00194.png => img_00344.png} (100%)\n",
            " copy data/subset/images/{img_00195.png => img_00345.png} (100%)\n",
            " rename data/subset/images/{img_00196.png => img_00346.png} (100%)\n",
            " copy data/subset/images/{img_00197.png => img_00347.png} (100%)\n",
            " copy data/subset/images/{img_00198.png => img_00348.png} (100%)\n",
            " copy data/subset/images/{img_00199.png => img_00349.png} (100%)\n",
            " create mode 100644 data/subset/images/img_00350.png\n",
            " create mode 100644 data/subset/images/img_00351.png\n",
            " create mode 100644 data/subset/images/img_00352.png\n",
            " create mode 100644 data/subset/images/img_00353.png\n",
            " create mode 100644 data/subset/images/img_00354.png\n",
            " create mode 100644 data/subset/images/img_00355.png\n",
            " create mode 100644 data/subset/images/img_00356.png\n",
            " create mode 100644 data/subset/images/img_00357.png\n",
            " create mode 100644 data/subset/images/img_00358.png\n",
            " create mode 100644 data/subset/images/img_00359.png\n",
            " create mode 100644 data/subset/images/img_00360.png\n",
            " create mode 100644 data/subset/images/img_00361.png\n",
            " create mode 100644 data/subset/images/img_00362.png\n",
            " create mode 100644 data/subset/images/img_00363.png\n",
            " create mode 100644 data/subset/images/img_00364.png\n",
            " create mode 100644 data/subset/images/img_00365.png\n",
            " create mode 100644 data/subset/images/img_00366.png\n",
            " create mode 100644 data/subset/images/img_00367.png\n",
            " create mode 100644 data/subset/images/img_00368.png\n",
            " create mode 100644 data/subset/images/img_00369.png\n",
            " create mode 100644 data/subset/images/img_00370.png\n",
            " create mode 100644 data/subset/images/img_00371.png\n",
            " create mode 100644 data/subset/images/img_00372.png\n",
            " create mode 100644 data/subset/images/img_00373.png\n",
            " create mode 100644 data/subset/images/img_00374.png\n",
            " create mode 100644 data/subset/images/img_00375.png\n",
            " create mode 100644 data/subset/images/img_00376.png\n",
            " create mode 100644 data/subset/images/img_00377.png\n",
            " create mode 100644 data/subset/images/img_00378.png\n",
            " create mode 100644 data/subset/images/img_00379.png\n",
            " create mode 100644 data/subset/images/img_00380.png\n",
            " create mode 100644 data/subset/images/img_00381.png\n",
            " create mode 100644 data/subset/images/img_00382.png\n",
            " create mode 100644 data/subset/images/img_00383.png\n",
            " create mode 100644 data/subset/images/img_00384.png\n",
            " create mode 100644 data/subset/images/img_00385.png\n",
            " create mode 100644 data/subset/images/img_00386.png\n",
            " create mode 100644 data/subset/images/img_00387.png\n",
            " create mode 100644 data/subset/images/img_00388.png\n",
            " create mode 100644 data/subset/images/img_00389.png\n",
            " create mode 100644 data/subset/images/img_00390.png\n",
            " create mode 100644 data/subset/images/img_00391.png\n",
            " create mode 100644 data/subset/images/img_00392.png\n",
            " create mode 100644 data/subset/images/img_00393.png\n",
            " create mode 100644 data/subset/images/img_00394.png\n",
            " create mode 100644 data/subset/images/img_00395.png\n",
            " create mode 100644 data/subset/images/img_00396.png\n",
            " create mode 100644 data/subset/images/img_00397.png\n",
            " create mode 100644 data/subset/images/img_00398.png\n",
            " create mode 100644 data/subset/images/img_00399.png\n",
            " create mode 100644 data/subset/images/img_00400.png\n",
            " create mode 100644 data/subset/images/img_00401.png\n",
            " create mode 100644 data/subset/images/img_00402.png\n",
            " create mode 100644 data/subset/images/img_00403.png\n",
            " create mode 100644 data/subset/images/img_00404.png\n",
            " create mode 100644 data/subset/images/img_00405.png\n",
            " create mode 100644 data/subset/images/img_00406.png\n",
            " create mode 100644 data/subset/images/img_00407.png\n",
            " create mode 100644 data/subset/images/img_00408.png\n",
            " create mode 100644 data/subset/images/img_00409.png\n",
            " create mode 100644 data/subset/images/img_00410.png\n",
            " create mode 100644 data/subset/images/img_00411.png\n",
            " create mode 100644 data/subset/images/img_00412.png\n",
            " create mode 100644 data/subset/images/img_00413.png\n",
            " create mode 100644 data/subset/images/img_00414.png\n",
            " create mode 100644 data/subset/images/img_00415.png\n",
            " create mode 100644 data/subset/images/img_00416.png\n",
            " create mode 100644 data/subset/images/img_00417.png\n",
            " create mode 100644 data/subset/images/img_00418.png\n",
            " create mode 100644 data/subset/images/img_00419.png\n",
            " create mode 100644 data/subset/images/img_00420.png\n",
            " create mode 100644 data/subset/images/img_00421.png\n",
            " create mode 100644 data/subset/images/img_00422.png\n",
            " create mode 100644 data/subset/images/img_00423.png\n",
            " create mode 100644 data/subset/images/img_00424.png\n",
            " create mode 100644 data/subset/images/img_00425.png\n",
            " create mode 100644 data/subset/images/img_00426.png\n",
            " create mode 100644 data/subset/images/img_00427.png\n",
            " create mode 100644 data/subset/images/img_00428.png\n",
            " create mode 100644 data/subset/images/img_00429.png\n",
            " create mode 100644 data/subset/images/img_00430.png\n",
            " create mode 100644 data/subset/images/img_00431.png\n",
            " create mode 100644 data/subset/images/img_00432.png\n",
            " create mode 100644 data/subset/images/img_00433.png\n",
            " create mode 100644 data/subset/images/img_00434.png\n",
            " create mode 100644 data/subset/images/img_00435.png\n",
            " create mode 100644 data/subset/images/img_00436.png\n",
            " create mode 100644 data/subset/images/img_00437.png\n",
            " create mode 100644 data/subset/images/img_00438.png\n",
            " create mode 100644 data/subset/images/img_00439.png\n",
            " create mode 100644 data/subset/images/img_00440.png\n",
            " create mode 100644 data/subset/images/img_00441.png\n",
            " create mode 100644 data/subset/images/img_00442.png\n",
            " create mode 100644 data/subset/images/img_00443.png\n",
            " create mode 100644 data/subset/images/img_00444.png\n",
            " create mode 100644 data/subset/images/img_00445.png\n",
            " create mode 100644 data/subset/images/img_00446.png\n",
            " create mode 100644 data/subset/images/img_00447.png\n",
            " create mode 100644 data/subset/images/img_00448.png\n",
            " create mode 100644 data/subset/images/img_00449.png\n",
            " create mode 100644 data/subset/images/img_00450.png\n",
            " create mode 100644 data/subset/images/img_00451.png\n",
            " create mode 100644 data/subset/images/img_00452.png\n",
            " create mode 100644 data/subset/images/img_00453.png\n",
            " create mode 100644 data/subset/images/img_00454.png\n",
            " create mode 100644 data/subset/images/img_00455.png\n",
            " create mode 100644 data/subset/images/img_00456.png\n",
            " create mode 100644 data/subset/images/img_00457.png\n",
            " create mode 100644 data/subset/images/img_00458.png\n",
            " create mode 100644 data/subset/images/img_00459.png\n",
            " create mode 100644 data/subset/images/img_00460.png\n",
            " create mode 100644 data/subset/images/img_00461.png\n",
            " create mode 100644 data/subset/images/img_00462.png\n",
            " create mode 100644 data/subset/images/img_00463.png\n",
            " create mode 100644 data/subset/images/img_00464.png\n",
            " create mode 100644 data/subset/images/img_00465.png\n",
            " create mode 100644 data/subset/images/img_00466.png\n",
            " create mode 100644 data/subset/images/img_00467.png\n",
            " create mode 100644 data/subset/images/img_00468.png\n",
            " create mode 100644 data/subset/images/img_00469.png\n",
            " create mode 100644 data/subset/images/img_00470.png\n",
            " create mode 100644 data/subset/images/img_00471.png\n",
            " create mode 100644 data/subset/images/img_00472.png\n",
            " create mode 100644 data/subset/images/img_00473.png\n",
            " create mode 100644 data/subset/images/img_00474.png\n",
            " create mode 100644 data/subset/images/img_00475.png\n",
            " create mode 100644 data/subset/images/img_00476.png\n",
            " create mode 100644 data/subset/images/img_00477.png\n",
            " create mode 100644 data/subset/images/img_00478.png\n",
            " create mode 100644 data/subset/images/img_00479.png\n",
            " create mode 100644 data/subset/images/img_00480.png\n",
            " create mode 100644 data/subset/images/img_00481.png\n",
            " create mode 100644 data/subset/images/img_00482.png\n",
            " create mode 100644 data/subset/images/img_00483.png\n",
            " create mode 100644 data/subset/images/img_00484.png\n",
            " create mode 100644 data/subset/images/img_00485.png\n",
            " create mode 100644 data/subset/images/img_00486.png\n",
            " create mode 100644 data/subset/images/img_00487.png\n",
            " create mode 100644 data/subset/images/img_00488.png\n",
            " create mode 100644 data/subset/images/img_00489.png\n",
            " create mode 100644 data/subset/images/img_00490.png\n",
            " create mode 100644 data/subset/images/img_00491.png\n",
            " create mode 100644 data/subset/images/img_00492.png\n",
            " create mode 100644 data/subset/images/img_00493.png\n",
            " create mode 100644 data/subset/images/img_00494.png\n",
            " create mode 100644 data/subset/images/img_00495.png\n",
            " create mode 100644 data/subset/images/img_00496.png\n",
            " create mode 100644 data/subset/images/img_00497.png\n",
            " create mode 100644 data/subset/images/img_00498.png\n",
            " create mode 100644 data/subset/images/img_00499.png\n",
            " rewrite results/generated_images/gen_000.png (97%)\n",
            " rewrite results/generated_images/gen_001.png (98%)\n",
            " rewrite results/generated_images/gen_002.png (97%)\n",
            " rewrite results/generated_images/gen_003.png (97%)\n",
            " rewrite results/generated_images/gen_004.png (97%)\n",
            " rewrite results/generated_images/gen_005.png (98%)\n",
            " rewrite results/generated_images/gen_006.png (97%)\n",
            " rewrite results/generated_images/gen_007.png (97%)\n",
            " rewrite results/generated_images/gen_008.png (98%)\n",
            " rewrite results/generated_images/gen_009.png (97%)\n",
            " create mode 100644 results/generated_images/gen_010.png\n",
            " create mode 100644 results/generated_images/gen_011.png\n",
            " create mode 100644 results/generated_images/gen_012.png\n",
            " create mode 100644 results/generated_images/gen_013.png\n",
            " create mode 100644 results/generated_images/gen_014.png\n",
            " create mode 100644 results/generated_images/gen_015.png\n",
            " create mode 100644 results/generated_images/gen_016.png\n",
            " create mode 100644 results/generated_images/gen_017.png\n",
            " create mode 100644 results/generated_images/gen_018.png\n",
            " create mode 100644 results/generated_images/gen_019.png\n",
            " create mode 100644 results/generated_images/gen_020.png\n",
            " create mode 100644 results/generated_images/gen_021.png\n",
            " create mode 100644 results/generated_images/gen_022.png\n",
            " create mode 100644 results/generated_images/gen_023.png\n",
            " create mode 100644 results/generated_images/gen_024.png\n",
            " create mode 100644 results/generated_images/gen_025.png\n",
            " create mode 100644 results/generated_images/gen_026.png\n",
            " create mode 100644 results/generated_images/gen_027.png\n",
            " create mode 100644 results/generated_images/gen_028.png\n",
            " create mode 100644 results/generated_images/gen_029.png\n",
            " create mode 100644 results/pipeline_summary.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass, os\n",
        "token = getpass.getpass(\"Paste your GitHub PAT (input hidden): \")\n",
        "os.environ[\"GITHUB_TOKEN\"] = token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiiZXzilhqvb",
        "outputId": "e1f785a5-9ace-43e1-c2c9-60cfc54d68ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste your GitHub PAT (input hidden): \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote -v\n",
        "!git remote set-url origin https://$GITHUB_TOKEN@github.com/joaquintb/vdm_prueba.git\n",
        "!git push"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kzeI3uxh5SW",
        "outputId": "364f8ba7-b3dd-4be3-9427-23f6edbe52e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "origin\thttps://github.com/joaquintb/vdm_prueba.git (fetch)\n",
            "origin\thttps://github.com/joaquintb/vdm_prueba.git (push)\n",
            "Enumerating objects: 396, done.\n",
            "Counting objects: 100% (396/396), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (338/338), done.\n",
            "Writing objects: 100% (339/339), 5.06 MiB | 12.83 MiB/s, done.\n",
            "Total 339 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To https://github.com/joaquintb/vdm_prueba.git\n",
            "   6fa1439..ea82d57  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pulling"
      ],
      "metadata": {
        "id": "ug9qNDzE-whn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPAyvjng-yUZ",
        "outputId": "a86c830e-aa5a-4b7c-de5a-d0467972e058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects:   5% (1/18)\u001b[K\rremote: Counting objects:  11% (2/18)\u001b[K\rremote: Counting objects:  16% (3/18)\u001b[K\rremote: Counting objects:  22% (4/18)\u001b[K\rremote: Counting objects:  27% (5/18)\u001b[K\rremote: Counting objects:  33% (6/18)\u001b[K\rremote: Counting objects:  38% (7/18)\u001b[K\rremote: Counting objects:  44% (8/18)\u001b[K\rremote: Counting objects:  50% (9/18)\u001b[K\rremote: Counting objects:  55% (10/18)\u001b[K\rremote: Counting objects:  61% (11/18)\u001b[K\rremote: Counting objects:  66% (12/18)\u001b[K\rremote: Counting objects:  72% (13/18)\u001b[K\rremote: Counting objects:  77% (14/18)\u001b[K\rremote: Counting objects:  83% (15/18)\u001b[K\rremote: Counting objects:  88% (16/18)\u001b[K\rremote: Counting objects:  94% (17/18)\u001b[K\rremote: Counting objects: 100% (18/18)\u001b[K\rremote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects:  12% (1/8)\u001b[K\rremote: Compressing objects:  25% (2/8)\u001b[K\rremote: Compressing objects:  37% (3/8)\u001b[K\rremote: Compressing objects:  50% (4/8)\u001b[K\rremote: Compressing objects:  62% (5/8)\u001b[K\rremote: Compressing objects:  75% (6/8)\u001b[K\rremote: Compressing objects:  87% (7/8)\u001b[K\rremote: Compressing objects: 100% (8/8)\u001b[K\rremote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 13 (delta 8), reused 10 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:   7% (1/13)\rUnpacking objects:  15% (2/13)\rUnpacking objects:  23% (3/13)\rUnpacking objects:  30% (4/13)\rUnpacking objects:  38% (5/13)\rUnpacking objects:  46% (6/13)\rUnpacking objects:  53% (7/13)\rUnpacking objects:  61% (8/13)\rUnpacking objects:  69% (9/13)\rUnpacking objects:  76% (10/13)\rUnpacking objects:  84% (11/13)\rUnpacking objects:  92% (12/13)\rUnpacking objects: 100% (13/13)\rUnpacking objects: 100% (13/13), 2.49 KiB | 510.00 KiB/s, done.\n",
            "From https://github.com/joaquintb/vdm_prueba\n",
            "   e03875a..7eec749  main       -> origin/main\n",
            "Updating e03875a..7eec749\n",
            "Fast-forward\n",
            " .gitignore                     |   2 \u001b[32m+\u001b[m\n",
            " src/generation/lcm_generate.py | 111 \u001b[32m++++++++++++++++++++++++++++++\u001b[m\u001b[31m-----------\u001b[m\n",
            " 2 files changed, 83 insertions(+), 30 deletions(-)\n"
          ]
        }
      ]
    }
  ]
}